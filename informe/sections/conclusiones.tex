\documentclass[../informe2.tex]{subfiles}
\begin{document}
%Conclusiones RELEVANTES del estudio realizado. Debería responder a las preguntas: todas las técnicas resuelven el mismo problema o hay algunas diferencias?, En qué se parecen o difieren las técnicas en el contexto del problema?, qué limitaciones tienen?, qué técnicas o estrategias son las más prometedoras?, existe trabajo futuro por realizar?, qué ideas usted propone como lineamientos para continuar con investigaciones futuras?

En el presente informe se ha realizado una descripción en detalle del \mrp, con una revisión de la literatura referente a las distintos algoritmos y técnicas desarrollados para la resolución del problema, considerando las metaheurísticas y representaciones utilizadas para aquello. \\
Cabe destacar que a pesar de lo novel del problema -publicado en la ROADEF/EURO 2012-, la naturaleza o estructura de éste no es algo nuevo en la investigación científica, puesto que el \mrp\ puede considerarse como una versión más restringida del \textit{Generalized Assignment Problem} o del \textit{Bin Packing Problem}. Sin embargo, dado el contexto actual en que los \textit{data centers} son piedras angulares en el funcionamiento de las organizaciones modernas y su gestión se vuelve una tarea sumamente crítica en la búsqueda de la efectividad y eficiencia, el \mrp\ por sí solo adquiere una relevancia importante. El hecho de que haya sido planteado por Google, corporación que basa la provisión de sus servicios a través del funcionamiento de extensas \textit{granjas} de servidores, es un indicativo de que para la industria de la ``nube'', los objetivos planteados por el  \mrp\ son críticos. \\
Respecto de las técnicas de resolución vistas en la sección~\ref{sec:estado}, todas intentan solucionar el problema original publicado por Google. En general, dado que la presentación del problema se puede dar a través de instancias de gran tamaño, considerando por ejemplo 5000 máquinas, 50000 procesos y 12 recursos (descripción bastante cercana de la realidad que afrontan los gigantes informáticos), los algoritmos presentados utilizan esquemas de búsqueda local, que se aplican de forma iterativa sobre problemas de menor tamaño que el original, obtenidos a partir de un subconjunto de las máquinas y procesos involucrados, tratando de buscar asignaciones factibles y de bajo costo. A esto se suma que, el desafío original imponía un tiempo límite de ejecución para encontrar una solución de cinco minutos, por lo que los algoritmos en cuestión deben ser rápidos en la entrega de resultados, por lo que la utilización técnicas completas de búsqueda queda totalmente descartada. Esto se sustenta en el hecho de que los \textit{data centers} funcionan 24/7, por lo que el tiempo disponible para generar asignaciones que mejoren el uso de los servidores, es bastante limitado. Otro aspecto relevante y relacionado con el tamaño de las instancias del problema, es el de la representación. Como lo dejó establecido el trabajo de Mehta et al.~\cite{mehta2012comparing}, la utilización de esquemas basados en \textit{Mixed Integer Programming} es bastante ineficiente para instancias de gran tamaño (con un espacio de búsqueda mayor, definición de una mayor cantidad de restricciones), por lo que la utilización del \textit{Constraint Programming} se es necesario para alcanzar una mayor escabilidad, especialmente cuando se utiliza una heurítica basada en LNS, combinación que entrega muy buenos resultados. Sin embargo, varios de los algoritmos vistos en el \textit{estado del arte} utilizan una representación basada en MIP (\cite{jaskowskihybrid},~\cite{lopes2015heuristics},~\cite{masson2013iterated}), valiéndose del hecho de que dentro de ejecución iterativa, se resuelven subproblemas de menor tamaño. Estos problemas MIP, son resueltos luego mediante algún solver como CPLEX.\\
En relación al trabajo futuro, es destacable las mejoras que puede proveer la utilización de múltiples procesadores a la hora de ejecutar los distintos algoritmos. También sería importante pensar en la configuración automática y en tiempo real de los distintos parámetros de entrada que necesitan los algoritmos para ejecutarse, como por ejemplo la proporción de máquinas que se utilizarán para crear un subproblema.


% Conclusiones RELEVANTES del estudio realizado. Debería contener conclusiones del informe 1 junto con conclusiones obtenidas del desarrollo del software, análisis de resultados de experimentos y trabajo futuro.

%CONCLUSION: Por último, cabe señalar que el análisis previo realizado sobre las características de los servicios, en torno al mínimo de dispersión y de cantidad de dependencias, puede resultar demasiado específico en función de las instancias utilizadas para realizar las distintas pruebas presentadas en este informe. Las instancias son generadas de forma aleatoria y nada asegura que la mayoría de los servicios generados posean las características antes mencionadas.


\end{document}
